{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e617cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GEMINI_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90089bba",
   "metadata": {},
   "source": [
    "# GEPA Prompt Optimization with Images\n",
    "\n",
    "This notebook demonstrates how to use GEPA with DSPy adapter to optimize prompts for image-based tasks.\n",
    "\n",
    "## Setup\n",
    "- **Task LLM**: Gemini 2.0 Flash Lite\n",
    "- **Reflection LLM**: GPT-5 (high reasoning)\n",
    "- **Dataset**: Images + Prompts ‚Üí Expected JSON output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f34d279",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Quick Start: Single-Worksheet Optimization\n",
    "\n",
    "**This notebook optimizes ONE worksheet type at a time.**\n",
    "\n",
    "### Current Configuration:\n",
    "- **Worksheet Index**: Set in cell 16 (`WORKSHEET_INDEX = 1`)\n",
    "- **Optimization Mode**: Individual (no generalization)\n",
    "- **Iterations**: 100 (adjustable in cell 24)\n",
    "\n",
    "### To Optimize Different Worksheets:\n",
    "1. **Worksheet 1**: Set `WORKSHEET_INDEX = 1` ‚Üí Run All\n",
    "2. **Worksheet 2**: Set `WORKSHEET_INDEX = 2` ‚Üí Run All  \n",
    "3. **Worksheet 3**: Set `WORKSHEET_INDEX = 3` ‚Üí Run All\n",
    "\n",
    "Each worksheet gets its own optimized program file.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a35ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "#!pip install gepa dspy-ai pillow openai google-generativeai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a1b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import dspy\n",
    "from gepa import optimize\n",
    "from gepa.adapters.dspy_full_program_adapter.full_program_adapter import DspyAdapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98000227",
   "metadata": {},
   "source": [
    "## Dataset Structure\n",
    "\n",
    "The Dataset folder should have the following structure:\n",
    "```\n",
    "Dataset/\n",
    "‚îú‚îÄ‚îÄ prompts/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 1.txt\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 2.txt\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ outputs/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 1.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 2.json\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ images/\n",
    "    ‚îú‚îÄ‚îÄ 1-a.png (or .jpg, .jpeg)\n",
    "    ‚îú‚îÄ‚îÄ 1-b.png (or .jpg, .jpeg)\n",
    "    ‚îú‚îÄ‚îÄ 2-a.jpg\n",
    "    ‚îú‚îÄ‚îÄ 2-b.jpg\n",
    "    ‚îî‚îÄ‚îÄ ...\n",
    "```\n",
    "\n",
    "**Supported image formats**: PNG, JPG, JPEG (case-insensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_to_base64(image_path: str) -> str:\n",
    "    \"\"\"Convert image to base64 string for API consumption. Supports PNG, JPG, JPEG.\"\"\"\n",
    "    with Image.open(image_path) as img:\n",
    "        # Convert RGBA to RGB if necessary (for JPG compatibility)\n",
    "        if img.mode == 'RGBA':\n",
    "            # Create white background\n",
    "            background = Image.new('RGB', img.size, (255, 255, 255))\n",
    "            background.paste(img, mask=img.split()[3])  # Use alpha channel as mask\n",
    "            img = background\n",
    "        \n",
    "        buffered = BytesIO()\n",
    "        # Detect format from file extension\n",
    "        img_format = Path(image_path).suffix.lower()\n",
    "        if img_format in ['.jpg', '.jpeg']:\n",
    "            img.save(buffered, format=\"JPEG\", quality=95)\n",
    "        else:\n",
    "            img.save(buffered, format=\"PNG\")\n",
    "        return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "\n",
    "def load_dataset_item(idx: int, dataset_path: str = \"Dataset\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load a single dataset item with prompt, images, and expected output.\n",
    "    \n",
    "    Args:\n",
    "        idx: Item index (1-based)\n",
    "        dataset_path: Path to Dataset folder\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing prompt, images, and expected JSON output\n",
    "    \"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    # Load prompt\n",
    "    prompt_file = dataset_path / \"prompts\" / f\"{idx}.txt\"\n",
    "    with open(prompt_file, 'r', encoding='utf-8') as f:\n",
    "        prompt = f.read().strip()\n",
    "    \n",
    "    # Load expected output\n",
    "    output_file = dataset_path / \"outputs\" / f\"{idx}.json\"\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        expected_output = json.load(f)\n",
    "    \n",
    "    # Load images (find all images for this index - supports PNG, JPG, JPEG)\n",
    "    images_dir = dataset_path / \"images\"\n",
    "    image_files = []\n",
    "    for ext in ['*.png', '*.jpg', '*.jpeg', '*.PNG', '*.JPG', '*.JPEG']:\n",
    "        image_files.extend(images_dir.glob(f\"{idx}-{ext}\"))\n",
    "    \n",
    "    # Remove duplicates (case-insensitive filesystem can cause duplicates)\n",
    "    seen = set()\n",
    "    unique_files = []\n",
    "    for f in sorted(image_files):\n",
    "        if f.name.lower() not in seen:\n",
    "            seen.add(f.name.lower())\n",
    "            unique_files.append(f)\n",
    "    image_files = unique_files\n",
    "    \n",
    "    images = []\n",
    "    for img_path in image_files:\n",
    "        images.append({\n",
    "            \"path\": str(img_path),\n",
    "            \"base64\": encode_image_to_base64(str(img_path))\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"index\": idx,\n",
    "        \"prompt\": prompt,\n",
    "        \"images\": images,\n",
    "        \"expected_output\": expected_output\n",
    "    }\n",
    "\n",
    "\n",
    "def load_all_dataset(dataset_path: str = \"Dataset\", max_items: int = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load all dataset items.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to Dataset folder\n",
    "        max_items: Maximum number of items to load (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        List of dataset items\n",
    "    \"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "    prompts_dir = dataset_path / \"prompts\"\n",
    "    \n",
    "    # Get all prompt files\n",
    "    prompt_files = sorted(prompts_dir.glob(\"*.txt\"))\n",
    "    indices = [int(f.stem) for f in prompt_files]\n",
    "    \n",
    "    if max_items:\n",
    "        indices = indices[:max_items]\n",
    "    \n",
    "    dataset = []\n",
    "    for idx in indices:\n",
    "        try:\n",
    "            item = load_dataset_item(idx, str(dataset_path))\n",
    "            dataset.append(item)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading item {idx}: {e}\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2af88c",
   "metadata": {},
   "source": [
    "## Validate Dataset Structure\n",
    "\n",
    "Run this to check if your Dataset folder is correctly set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b370cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset(dataset_path: str = \"Dataset\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate the Dataset folder structure and contents.\n",
    "    \n",
    "    Checks:\n",
    "    - Folder structure exists\n",
    "    - Prompts, outputs, and images are properly matched\n",
    "    - File formats are correct\n",
    "    - JSON files are valid\n",
    "    - Images can be loaded\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with validation results and issues found\n",
    "    \"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "    results = {\n",
    "        \"valid\": True,\n",
    "        \"issues\": [],\n",
    "        \"warnings\": [],\n",
    "        \"summary\": {},\n",
    "        \"worksheets\": {}\n",
    "    }\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"DATASET VALIDATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check if Dataset folder exists\n",
    "    if not dataset_path.exists():\n",
    "        results[\"valid\"] = False\n",
    "        results[\"issues\"].append(f\"‚ùå Dataset folder not found: {dataset_path}\")\n",
    "        print(f\"‚ùå Dataset folder not found: {dataset_path}\")\n",
    "        return results\n",
    "    \n",
    "    print(f\"‚úì Dataset folder found: {dataset_path}\")\n",
    "    \n",
    "    # Check required subfolders\n",
    "    prompts_dir = dataset_path / \"prompts\"\n",
    "    outputs_dir = dataset_path / \"outputs\"\n",
    "    images_dir = dataset_path / \"images\"\n",
    "    \n",
    "    for folder_name, folder_path in [(\"prompts\", prompts_dir), (\"outputs\", outputs_dir), (\"images\", images_dir)]:\n",
    "        if not folder_path.exists():\n",
    "            results[\"valid\"] = False\n",
    "            results[\"issues\"].append(f\"‚ùå Missing '{folder_name}' folder\")\n",
    "            print(f\"‚ùå Missing '{folder_name}' folder\")\n",
    "        else:\n",
    "            print(f\"‚úì Found '{folder_name}' folder\")\n",
    "    \n",
    "    if not results[\"valid\"]:\n",
    "        return results\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"CHECKING WORKSHEETS\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Get all prompt files\n",
    "    prompt_files = sorted(prompts_dir.glob(\"*.txt\"))\n",
    "    \n",
    "    if not prompt_files:\n",
    "        results[\"valid\"] = False\n",
    "        results[\"issues\"].append(\"‚ùå No prompt files (.txt) found in prompts folder\")\n",
    "        print(\"‚ùå No prompt files (.txt) found in prompts folder\")\n",
    "        return results\n",
    "    \n",
    "    print(f\"\\n‚úì Found {len(prompt_files)} prompt file(s)\")\n",
    "    \n",
    "    # Check each worksheet\n",
    "    for prompt_file in prompt_files:\n",
    "        worksheet_idx = prompt_file.stem\n",
    "        worksheet_info = {\n",
    "            \"prompt_file\": str(prompt_file),\n",
    "            \"issues\": [],\n",
    "            \"warnings\": [],\n",
    "            \"valid\": True\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Worksheet: {worksheet_idx}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Check prompt file\n",
    "        try:\n",
    "            with open(prompt_file, 'r', encoding='utf-8') as f:\n",
    "                prompt_content = f.read().strip()\n",
    "            \n",
    "            if not prompt_content:\n",
    "                worksheet_info[\"issues\"].append(\"‚ö†Ô∏è  Prompt file is empty\")\n",
    "                worksheet_info[\"valid\"] = False\n",
    "                print(f\"‚ö†Ô∏è  Prompt file is empty\")\n",
    "            else:\n",
    "                print(f\"‚úì Prompt file: {len(prompt_content)} characters\")\n",
    "                worksheet_info[\"prompt_length\"] = len(prompt_content)\n",
    "        except Exception as e:\n",
    "            worksheet_info[\"issues\"].append(f\"‚ùå Error reading prompt file: {e}\")\n",
    "            worksheet_info[\"valid\"] = False\n",
    "            print(f\"‚ùå Error reading prompt file: {e}\")\n",
    "        \n",
    "        # Check output JSON file\n",
    "        output_file = outputs_dir / f\"{worksheet_idx}.json\"\n",
    "        if not output_file.exists():\n",
    "            worksheet_info[\"issues\"].append(\"‚ùå Missing corresponding .json file in outputs folder\")\n",
    "            worksheet_info[\"valid\"] = False\n",
    "            print(f\"‚ùå Missing corresponding .json file in outputs folder\")\n",
    "        else:\n",
    "            try:\n",
    "                with open(output_file, 'r', encoding='utf-8') as f:\n",
    "                    output_data = json.load(f)\n",
    "                \n",
    "                if not output_data:\n",
    "                    worksheet_info[\"warnings\"].append(\"‚ö†Ô∏è  Output JSON is empty\")\n",
    "                    print(f\"‚ö†Ô∏è  Output JSON is empty\")\n",
    "                else:\n",
    "                    # Handle both list and dict formats\n",
    "                    if isinstance(output_data, list):\n",
    "                        print(f\"‚úì Output JSON: List with {len(output_data)} items\")\n",
    "                        if output_data and isinstance(output_data[0], dict):\n",
    "                            print(f\"  First item keys: {list(output_data[0].keys())}\")\n",
    "                            worksheet_info[\"output_type\"] = \"list\"\n",
    "                            worksheet_info[\"output_count\"] = len(output_data)\n",
    "                    elif isinstance(output_data, dict):\n",
    "                        print(f\"‚úì Output JSON: Dict with {len(output_data)} key(s) - {list(output_data.keys())}\")\n",
    "                        worksheet_info[\"output_keys\"] = list(output_data.keys())\n",
    "                        worksheet_info[\"output_type\"] = \"dict\"\n",
    "            except json.JSONDecodeError as e:\n",
    "                worksheet_info[\"issues\"].append(f\"‚ùå Invalid JSON format: {e}\")\n",
    "                worksheet_info[\"valid\"] = False\n",
    "                print(f\"‚ùå Invalid JSON format: {e}\")\n",
    "            except Exception as e:\n",
    "                worksheet_info[\"issues\"].append(f\"‚ùå Error reading output file: {e}\")\n",
    "                worksheet_info[\"valid\"] = False\n",
    "                print(f\"‚ùå Error reading output file: {e}\")\n",
    "        \n",
    "        # Check image files\n",
    "        image_files = []\n",
    "        for ext in ['*.png', '*.jpg', '*.jpeg', '*.PNG', '*.JPG', '*.JPEG']:\n",
    "            image_files.extend(images_dir.glob(f\"{worksheet_idx}-{ext}\"))\n",
    "        \n",
    "        # Remove duplicates (case-insensitive filesystem can cause duplicates)\n",
    "        seen = set()\n",
    "        unique_files = []\n",
    "        for f in sorted(image_files):\n",
    "            if f.name.lower() not in seen:\n",
    "                seen.add(f.name.lower())\n",
    "                unique_files.append(f)\n",
    "        image_files = unique_files\n",
    "        \n",
    "        if not image_files:\n",
    "            worksheet_info[\"issues\"].append(\"‚ùå No image files found (expected format: {worksheet_idx}-a.png, {worksheet_idx}-b.jpg, etc.)\")\n",
    "            worksheet_info[\"valid\"] = False\n",
    "            print(f\"‚ùå No image files found (expected format: {worksheet_idx}-a.png, {worksheet_idx}-b.jpg, etc.)\")\n",
    "        else:\n",
    "            print(f\"‚úì Found {len(image_files)} image(s):\")\n",
    "            worksheet_info[\"images\"] = []\n",
    "            \n",
    "            for img_file in image_files:\n",
    "                try:\n",
    "                    # Try to open and validate image\n",
    "                    with Image.open(img_file) as img:\n",
    "                        img_info = {\n",
    "                            \"file\": img_file.name,\n",
    "                            \"format\": img.format,\n",
    "                            \"size\": f\"{img.width}x{img.height}\",\n",
    "                            \"mode\": img.mode\n",
    "                        }\n",
    "                        worksheet_info[\"images\"].append(img_info)\n",
    "                        print(f\"  ‚úì {img_file.name} - {img.format} {img.width}x{img.height} ({img.mode})\")\n",
    "                except Exception as e:\n",
    "                    worksheet_info[\"issues\"].append(f\"‚ùå Error loading image {img_file.name}: {e}\")\n",
    "                    worksheet_info[\"valid\"] = False\n",
    "                    print(f\"  ‚ùå Error loading image {img_file.name}: {e}\")\n",
    "        \n",
    "        # Store worksheet info\n",
    "        results[\"worksheets\"][worksheet_idx] = worksheet_info\n",
    "        \n",
    "        if not worksheet_info[\"valid\"]:\n",
    "            results[\"valid\"] = False\n",
    "        \n",
    "        # Summary for this worksheet\n",
    "        if worksheet_info[\"valid\"]:\n",
    "            print(f\"\\n‚úÖ Worksheet {worksheet_idx} is valid!\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Worksheet {worksheet_idx} has issues!\")\n",
    "            print(f\"\\n‚ùå Worksheet {worksheet_idx} has issues!\")\n",
    "    # Overall summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VALIDATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    valid_worksheets = sum(1 for w in results[\"worksheets\"].values() if w[\"valid\"])\n",
    "    total_worksheets = len(results[\"worksheets\"])\n",
    "    \n",
    "    results[\"summary\"] = {\n",
    "        \"total_worksheets\": total_worksheets,\n",
    "        \"valid_worksheets\": valid_worksheets,\n",
    "        \"invalid_worksheets\": total_worksheets - valid_worksheets\n",
    "    }\n",
    "    \n",
    "    print(f\"Total Worksheets: {total_worksheets}\")\n",
    "    print(f\"Valid: {valid_worksheets}\")\n",
    "    print(f\"Invalid: {total_worksheets - valid_worksheets}\")\n",
    "    \n",
    "    if results[\"valid\"]:\n",
    "        print(f\"\\n‚úÖ All worksheets are valid! Ready for optimization.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Some worksheets have issues. Please fix them before optimization.\")\n",
    "        print(f\"\\nIssues found:\")\n",
    "        for ws_idx, ws_info in results[\"worksheets\"].items():\n",
    "            if ws_info[\"issues\"]:\n",
    "                print(f\"\\n  Worksheet {ws_idx}:\")\n",
    "                for issue in ws_info[\"issues\"]:\n",
    "                    print(f\"    {issue}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #     print(validation_results[\"worksheets\"][\"1\"])# # if \"1\" in validation_results[\"worksheets\"]:# # To check a specific worksheet:# validation_results = validate_dataset(\"Dataset\")# Run validation    return results\n",
    "\n",
    "\n",
    "# Run validation\n",
    "validation_results = validate_dataset(\"Dataset\")\n",
    "# \n",
    "# # To check a specific worksheet:\n",
    "# # if \"1\" in validation_results[\"worksheets\"]:\n",
    "# #     print(validation_results[\"worksheets\"][\"1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce5b402",
   "metadata": {},
   "source": [
    "## ‚ö° Quick Validation Check\n",
    "\n",
    "Run this cell to validate your Dataset folder before optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67a1899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Dataset structure\n",
    "validation_results = validate_dataset(\"Dataset\")\n",
    "\n",
    "# Store for later reference\n",
    "dataset_is_valid = validation_results[\"valid\"]\n",
    "\n",
    "if dataset_is_valid:\n",
    "    print(\"\\nüéâ Your dataset is ready for optimization!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Please fix the issues above before running optimization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f297bd1",
   "metadata": {},
   "source": [
    "## DSPy Program for Image Processing\n",
    "\n",
    "This defines a DSPy program that takes images and a prompt, processes them with vision-capable LLM, and returns JSON output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31d7284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial seed program for GEPA optimization\n",
    "# This uses a custom vision module that actually sends images to the vision API\n",
    "seed_program = \"\"\"\n",
    "import dspy\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ExtractedQuestion(BaseModel):\n",
    "    question_number: int = Field(description=\"The unique identifier for the question\")\n",
    "    question: str = Field(description=\"The entire text of the question without the student's answer\")\n",
    "    student_answer: str = Field(description=\"The student's answer exactly as written\")\n",
    "\n",
    "class VisionImageAnalysis(dspy.Signature):\n",
    "    '''Extract questions and student answers from worksheet images using vision capabilities.\n",
    "    \n",
    "    Analyze the provided worksheet images and extract EVERY question with:\n",
    "    - question_number: Sequential number (1, 2, 3, ...)\n",
    "    - question: The COMPLETE question text (e.g., \"Q1. __ , 31\" not \"What comes before 31?\")\n",
    "    - student_answer: EXACTLY what the student wrote in pencil\n",
    "    \n",
    "    Return ONLY a valid JSON array. Do NOT add any explanatory text.\n",
    "    '''\n",
    "    \n",
    "    prompt: str = dspy.InputField(desc=\"Instructions for analyzing the worksheet images\")\n",
    "    images_info: str = dspy.InputField(desc=\"Metadata about the images being analyzed\")\n",
    "    json_output: str = dspy.OutputField(desc=\"JSON array containing extracted questions and answers\")\n",
    "\n",
    "class ImageAnalysisProgram(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.signature = VisionImageAnalysis\n",
    "    \n",
    "    def forward(self, prompt: str, images_info: str, images: List[Dict] = None):\n",
    "        '''Execute vision analysis with actual image data'''\n",
    "        if not images:\n",
    "            return dspy.Prediction(json_output='[]')\n",
    "        \n",
    "        # Call OpenAI vision API with structured output\n",
    "        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "        \n",
    "        # Build message with images\n",
    "        content = [{\"type\": \"text\", \"text\": prompt}]\n",
    "        \n",
    "        for img in images:\n",
    "            img_path = Path(img.get('path', ''))\n",
    "            ext = img_path.suffix.lower()\n",
    "            mime_type = \"image/jpeg\" if ext in ['.jpg', '.jpeg'] else \"image/png\"\n",
    "            \n",
    "            content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:{mime_type};base64,{img['base64']}\"}\n",
    "            })\n",
    "        \n",
    "        # Call with response_format for JSON\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": content}],\n",
    "            temperature=0.1,  # Lower for more consistent JSON\n",
    "            max_tokens=4096,\n",
    "            response_format={\"type\": \"json_object\"}  # Force JSON output\n",
    "        )\n",
    "        \n",
    "        output = response.choices[0].message.content\n",
    "        \n",
    "        # Try to extract JSON array if wrapped\n",
    "        output = output.strip()\n",
    "        \n",
    "        # If output is wrapped in a JSON object, extract the array\n",
    "        try:\n",
    "            parsed = json.loads(output)\n",
    "            if isinstance(parsed, dict) and 'questions' in parsed:\n",
    "                output = json.dumps(parsed['questions'])\n",
    "            elif isinstance(parsed, dict) and 'results' in parsed:\n",
    "                output = json.dumps(parsed['results'])\n",
    "            elif isinstance(parsed, dict) and 'data' in parsed:\n",
    "                output = json.dumps(parsed['data'])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return dspy.Prediction(json_output=output)\n",
    "\n",
    "program = ImageAnalysisProgram()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3def5153",
   "metadata": {},
   "source": [
    "## Execution Function with Vision API\n",
    "\n",
    "This function executes the prompt with images using vision-capable models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d65f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_vision_prompt(prompt: str, images: List[Dict], system_prompt: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Execute a prompt with images using OpenAI's vision API (gpt-4o).\n",
    "    \n",
    "    Args:\n",
    "        prompt: The user prompt\n",
    "        images: List of image dictionaries with 'base64' field\n",
    "        system_prompt: Optional system prompt for context\n",
    "        \n",
    "    Returns:\n",
    "        JSON string output from the model\n",
    "    \"\"\"\n",
    "    from openai import OpenAI\n",
    "    \n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    # Build message content with images\n",
    "    content = []\n",
    "    \n",
    "    if system_prompt:\n",
    "        content.append({\"type\": \"text\", \"text\": system_prompt})\n",
    "    \n",
    "    content.append({\"type\": \"text\", \"text\": prompt})\n",
    "    \n",
    "    # Add all images with proper MIME type detection\n",
    "    for img in images:\n",
    "        # Detect image format from file path\n",
    "        img_path = Path(img.get('path', ''))\n",
    "        ext = img_path.suffix.lower()\n",
    "        \n",
    "        if ext in ['.jpg', '.jpeg']:\n",
    "            mime_type = \"image/jpeg\"\n",
    "        else:\n",
    "            mime_type = \"image/png\"\n",
    "        \n",
    "        content.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:{mime_type};base64,{img['base64']}\"\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Call vision API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # This is the flash-lite equivalent\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=4096\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def execute_with_gemini(prompt: str, images: List[Dict], system_prompt: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Alternative: Execute with Google Gemini (gpt-5 high equivalent for reasoning).\n",
    "    \n",
    "    Args:\n",
    "        prompt: The user prompt\n",
    "        images: List of image dictionaries with 'path' field\n",
    "        system_prompt: Optional system prompt for context\n",
    "        \n",
    "    Returns:\n",
    "        JSON string output from the model\n",
    "    \"\"\"\n",
    "    import google.generativeai as genai\n",
    "    \n",
    "    genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "    \n",
    "    # Use Gemini 2.0 Flash (high reasoning capability)\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash-exp')\n",
    "    \n",
    "    # Build content with images\n",
    "    content_parts = []\n",
    "    \n",
    "    if system_prompt:\n",
    "        content_parts.append(system_prompt + \"\\n\\n\")\n",
    "    \n",
    "    content_parts.append(prompt)\n",
    "    \n",
    "    # Add images\n",
    "    for img in images:\n",
    "        pil_img = Image.open(img['path'])\n",
    "        content_parts.append(pil_img)\n",
    "    \n",
    "    response = model.generate_content(content_parts)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f3e85d",
   "metadata": {},
   "source": [
    "## Metric Function for Evaluation\n",
    "\n",
    "This function compares the model output with expected JSON output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6307bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_json_outputs(predicted: str, expected) -> float:\n",
    "    \"\"\"\n",
    "    Compare predicted JSON output with expected output.\n",
    "    Supports both dict and list expected outputs.\n",
    "    \n",
    "    Args:\n",
    "        predicted: Predicted JSON string from model\n",
    "        expected: Expected JSON (dict or list)\n",
    "        \n",
    "    Returns:\n",
    "        Score between 0 and 1 (1 = perfect match)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse predicted JSON\n",
    "        predicted_json = json.loads(predicted)\n",
    "        \n",
    "        # Exact match check\n",
    "        if predicted_json == expected:\n",
    "            return 1.0\n",
    "        \n",
    "        # Handle list outputs (array of objects)\n",
    "        if isinstance(expected, list) and isinstance(predicted_json, list):\n",
    "            if len(expected) == 0:\n",
    "                return 0.0\n",
    "            \n",
    "            # Compare list elements\n",
    "            score = 0.0\n",
    "            max_len = max(len(expected), len(predicted_json))\n",
    "            \n",
    "            for i in range(min(len(expected), len(predicted_json))):\n",
    "                expected_item = expected[i]\n",
    "                predicted_item = predicted_json[i]\n",
    "                \n",
    "                if expected_item == predicted_item:\n",
    "                    score += 1.0 / max_len\n",
    "                elif isinstance(expected_item, dict) and isinstance(predicted_item, dict):\n",
    "                    # Partial credit for matching keys in dict items\n",
    "                    item_score = 0.0\n",
    "                    for key in expected_item:\n",
    "                        if key in predicted_item:\n",
    "                            if predicted_item[key] == expected_item[key]:\n",
    "                                item_score += 1.0 / len(expected_item)\n",
    "                            else:\n",
    "                                item_score += 0.3 / len(expected_item)\n",
    "                    score += item_score / max_len\n",
    "            \n",
    "            return score\n",
    "        \n",
    "        # Handle dict outputs\n",
    "        if isinstance(expected, dict) and isinstance(predicted_json, dict):\n",
    "            score = 0.0\n",
    "            total_keys = len(expected)\n",
    "            \n",
    "            if total_keys == 0:\n",
    "                return 0.0\n",
    "            \n",
    "            for key in expected:\n",
    "                if key in predicted_json:\n",
    "                    # Check if values match (for simple comparison)\n",
    "                    if predicted_json[key] == expected[key]:\n",
    "                        score += 1.0 / total_keys\n",
    "                    else:\n",
    "                        # Partial credit for having the key\n",
    "                        score += 0.5 / total_keys\n",
    "            \n",
    "            return score\n",
    "        \n",
    "        # Type mismatch\n",
    "        return 0.0\n",
    "        \n",
    "    except json.JSONDecodeError:\n",
    "        # Invalid JSON - try string matching for recovery\n",
    "        score = 0.0\n",
    "        if isinstance(expected, dict):\n",
    "            for key, value in expected.items():\n",
    "                if str(key) in predicted and str(value) in predicted:\n",
    "                    score += 0.3 / len(expected)\n",
    "        elif isinstance(expected, list):\n",
    "            # Check if any expected items appear as strings\n",
    "            for item in expected[:5]:  # Check first 5 items\n",
    "                if str(item) in predicted:\n",
    "                    score += 0.1 / min(len(expected), 5)\n",
    "        return score\n",
    "    except Exception as e:\n",
    "        print(f\"Error comparing outputs: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def metric_fn(example, pred, trace=None) -> float:\n",
    "    \"\"\"\n",
    "    Metric function for DSPy adapter.\n",
    "    \n",
    "    Args:\n",
    "        example: DSPy Example with expected_output field\n",
    "        pred: Prediction with json_output field\n",
    "        trace: Optional trace information\n",
    "        \n",
    "    Returns:\n",
    "        Score between 0 and 1\n",
    "    \"\"\"\n",
    "    try:\n",
    "        predicted_output = pred.json_output if hasattr(pred, 'json_output') else str(pred)\n",
    "        expected_output = example.expected_output if hasattr(example, 'expected_output') else {}\n",
    "        \n",
    "        return compare_json_outputs(predicted_output, expected_output)\n",
    "    except Exception as e:\n",
    "        print(f\"Metric evaluation error: {e}\")\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be42e7c",
   "metadata": {},
   "source": [
    "## Prepare Dataset for DSPy\n",
    "\n",
    "Convert dataset items to DSPy Examples with image context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3da1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dspy_dataset(dataset: List[Dict[str, Any]]) -> List[dspy.Example]:\n",
    "    \"\"\"\n",
    "    Convert dataset items to DSPy Examples.\n",
    "    \n",
    "    Args:\n",
    "        dataset: List of dataset items from load_all_dataset\n",
    "        \n",
    "    Returns:\n",
    "        List of DSPy Examples\n",
    "    \"\"\"\n",
    "    dspy_examples = []\n",
    "    \n",
    "    for item in dataset:\n",
    "        # Create image context description\n",
    "        images_info = f\"Number of images: {len(item['images'])}\"\n",
    "        for i, img in enumerate(item['images'], 1):\n",
    "            images_info += f\"\\n- Image {i}: {img['path']}\"\n",
    "        \n",
    "        # Create DSPy Example - IMPORTANT: Pass raw images so vision model can access them\n",
    "        example = dspy.Example(\n",
    "            prompt=item['prompt'],\n",
    "            images_info=images_info,\n",
    "            images=item['images'],  # Pass actual image data with base64\n",
    "            expected_output=item['expected_output']\n",
    "        ).with_inputs('prompt', 'images_info', 'images')  # Include 'images' as input\n",
    "        \n",
    "        dspy_examples.append(example)\n",
    "    \n",
    "    return dspy_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a096e4c",
   "metadata": {},
   "source": [
    "## Load and Split Dataset\n",
    "\n",
    "Load the dataset and split into train/validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185687b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION: Single Worksheet Optimization Mode\n",
    "# ============================================================\n",
    "# Set the worksheet index you want to optimize (e.g., 1, 2, 3, etc.)\n",
    "WORKSHEET_INDEX = 1  # Change this to optimize different worksheet types\n",
    "\n",
    "# Load single worksheet for optimization\n",
    "print(f\"Loading worksheet {WORKSHEET_INDEX} for individual optimization...\")\n",
    "single_item = load_dataset_item(WORKSHEET_INDEX, \"Dataset\")\n",
    "print(f\"‚úì Loaded worksheet {WORKSHEET_INDEX}\")\n",
    "print(f\"  - Prompt length: {len(single_item['prompt'])} chars\")\n",
    "print(f\"  - Number of images: {len(single_item['images'])}\")\n",
    "\n",
    "# Handle both list and dict expected outputs\n",
    "expected_output = single_item['expected_output']\n",
    "if isinstance(expected_output, list):\n",
    "    print(f\"  - Expected output: List with {len(expected_output)} items\")\n",
    "    if expected_output:\n",
    "        print(f\"  - First item keys: {list(expected_output[0].keys())}\")\n",
    "elif isinstance(expected_output, dict):\n",
    "    print(f\"  - Expected output keys: {list(expected_output.keys())}\")\n",
    "else:\n",
    "    print(f\"  - Expected output type: {type(expected_output)}\")\n",
    "\n",
    "# Convert to DSPy format\n",
    "dspy_dataset = prepare_dspy_dataset([single_item])\n",
    "\n",
    "# For single worksheet optimization: use the same example for both train and val\n",
    "# This allows GEPA to iterate and refine specifically for this worksheet type\n",
    "train_set = dspy_dataset  # Use for training/optimization\n",
    "val_set = dspy_dataset    # Use for validation/scoring\n",
    "\n",
    "print(f\"\\n‚úì Single-worksheet optimization mode enabled\")\n",
    "print(f\"  Training set: {len(train_set)} item(s)\")\n",
    "print(f\"  Validation set: {len(val_set)} item(s)\")\n",
    "print(f\"\\nGEPA will optimize the prompt specifically for worksheet type {WORKSHEET_INDEX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b19a8",
   "metadata": {},
   "source": [
    "## Configure Language Models\n",
    "\n",
    "Setup task LLM (Gemini 2.0 Flash Lite) and reflection LLM (GPT-5 for high reasoning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5690db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure DSPy with Gemini 2.0 Flash Lite for task execution\n",
    "task_lm = dspy.LM(\n",
    "    model=\"gemini/gemini-2.5-flash-lite\",  # Task LM - Gemini 2.0 Flash Lite\n",
    "    max_tokens=16000\n",
    ")\n",
    "\n",
    "# Configure reflection LM with GPT-5 (high reasoning capability)\n",
    "# Note: GPT-5 reasoning models require temperature=1.0 and max_tokens >= 16000\n",
    "reflection_lm = dspy.LM(\n",
    "    model=\"openai/gpt-5\",  # Reflection LM - GPT-5 for high reasoning\n",
    "    temperature=1.0,  # Required for reasoning models\n",
    "    max_tokens=16000,  # Minimum required for reasoning models\n",
    ")\n",
    "\n",
    "print(\"Language models configured:\")\n",
    "print(f\"- Task LM: {task_lm.model}\")\n",
    "print(f\"- Reflection LM: {reflection_lm.model} (reasoning mode: temp=1.0, max_tokens=16000)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a70a31",
   "metadata": {},
   "source": [
    "## Setup DSPy Adapter for GEPA\n",
    "\n",
    "Configure the DspyAdapter with the task LM, metric function, and reflection LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee3354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup DSPy Adapter\n",
    "adapter = DspyAdapter(\n",
    "    task_lm=task_lm,\n",
    "    metric_fn=metric_fn,\n",
    "    num_threads=4,  # Adjust based on your system\n",
    "    reflection_lm=lambda x: reflection_lm(x)[0]\n",
    ")\n",
    "\n",
    "print(\"DSPy Adapter configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ea2c7",
   "metadata": {},
   "source": [
    "## Test Baseline Performance\n",
    "\n",
    "Test the initial seed program before optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735448b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating baseline performance...\")\n",
    "baseline_result = adapter.evaluate(val_set, {\"program\": seed_program})\n",
    "print(f\"\\nBaseline performance: {baseline_result}\")\n",
    "\n",
    "# Access as object attributes, not dict\n",
    "if hasattr(baseline_result, 'scores') and baseline_result.scores:\n",
    "    avg_score = sum(baseline_result.scores) / len(baseline_result.scores)\n",
    "    print(f\"Average score: {avg_score:.4f}\")\n",
    "else:\n",
    "    print(f\"Average score: N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e46c7b",
   "metadata": {},
   "source": [
    "## Run GEPA Optimization\n",
    "\n",
    "Now run the GEPA optimization process to improve the prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d6e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting GEPA optimization for Worksheet {WORKSHEET_INDEX}...\")\n",
    "print(\"This will run multiple reasoning iterations to optimize specifically for this worksheet type...\")\n",
    "print(f\"Max iterations: 100\")\n",
    "\n",
    "# Run GEPA optimization\n",
    "optimization_result = optimize(\n",
    "    seed_candidate={\"program\": seed_program},\n",
    "    trainset=train_set,\n",
    "    valset=val_set,\n",
    "    adapter=adapter,\n",
    "    reflection_lm=lambda x: reflection_lm(x)[0],\n",
    "    max_metric_calls=3,  # Adjust for more/fewer optimization iterations\n",
    "    display_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"OPTIMIZATION COMPLETE FOR WORKSHEET {WORKSHEET_INDEX}!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8bd885",
   "metadata": {},
   "source": [
    "## View Optimized Program\n",
    "\n",
    "Let's see what GEPA came up with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5bd17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMIZED PROGRAM\")\n",
    "print(\"=\"*80)\n",
    "print(optimization_result.best_candidate[\"program\"])\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c0a66c",
   "metadata": {},
   "source": [
    "## Evaluate Optimized Performance\n",
    "\n",
    "Compare the optimized program against the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467decfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating optimized program...\")\n",
    "optimized_result = adapter.evaluate(val_set, optimization_result.best_candidate)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate scores from object attributes\n",
    "baseline_score = sum(baseline_result.scores) / len(baseline_result.scores) if baseline_result.scores else 0\n",
    "optimized_score = sum(optimized_result.scores) / len(optimized_result.scores) if optimized_result.scores else 0\n",
    "\n",
    "print(f\"Baseline Average Score:  {baseline_score:.4f}\")\n",
    "print(f\"Optimized Average Score: {optimized_score:.4f}\")\n",
    "\n",
    "if baseline_score > 0:\n",
    "    improvement = optimized_score - baseline_score\n",
    "    improvement_pct = (improvement / baseline_score) * 100\n",
    "    print(f\"\\nImprovement: {improvement:.4f} ({improvement_pct:.2f}%)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0e7723",
   "metadata": {},
   "source": [
    "## Test Function with Images\n",
    "\n",
    "Create a test function that executes a single prompt with images using the optimized program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bfff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_images(example_idx: int, use_optimized: bool = True):\n",
    "    \"\"\"\n",
    "    Test a single example from the dataset with images.\n",
    "    \n",
    "    Args:\n",
    "        example_idx: Index of example to test (0-based from validation set)\n",
    "        use_optimized: If True, use optimized program; otherwise use baseline\n",
    "    \"\"\"\n",
    "    if example_idx >= len(val_set):\n",
    "        print(f\"Error: Example index {example_idx} out of range (max: {len(val_set)-1})\")\n",
    "        return\n",
    "    \n",
    "    example = val_set[example_idx]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"TESTING EXAMPLE {example_idx}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nPrompt: {example.prompt}\")\n",
    "    print(f\"Number of images: {len(example.images)}\")\n",
    "    \n",
    "    # Display images\n",
    "    for i, img_data in enumerate(example.images, 1):\n",
    "        print(f\"  Image {i}: {img_data['path']}\")\n",
    "        img = Image.open(img_data['path'])\n",
    "        display(img.resize((min(img.width, 400), min(img.height, 400))))\n",
    "    \n",
    "    print(\"\\nExpected Output:\")\n",
    "    print(json.dumps(example.expected_output, indent=2))\n",
    "    \n",
    "    # Execute with selected program\n",
    "    program_code = optimization_result.best_candidate[\"program\"] if use_optimized else seed_program\n",
    "    program_type = \"OPTIMIZED\" if use_optimized else \"BASELINE\"\n",
    "    \n",
    "    print(f\"\\nExecuting with {program_type} program...\")\n",
    "    \n",
    "    # For simplicity, execute directly with vision API\n",
    "    # In production, you'd execute the DSPy program\n",
    "    result = execute_vision_prompt(\n",
    "        prompt=example.prompt,\n",
    "        images=example.images,\n",
    "        system_prompt=\"Analyze the images and return a valid JSON response matching the prompt requirements.\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel Output ({program_type}):\")\n",
    "    print(result)\n",
    "    \n",
    "    # Calculate score\n",
    "    score = compare_json_outputs(result, example.expected_output)\n",
    "    print(f\"\\nMatch Score: {score:.2%}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return result, score\n",
    "\n",
    "\n",
    "# Example usage - test first validation example\n",
    "# test_with_images(0, use_optimized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7823d86c",
   "metadata": {},
   "source": [
    "## Save Optimized Program\n",
    "\n",
    "Save the optimized program for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2bca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized program for this specific worksheet\n",
    "output_file = f\"optimized_worksheet_{WORKSHEET_INDEX}.py\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(optimization_result.best_candidate[\"program\"])\n",
    "\n",
    "print(f\"üíæ Optimized program saved to: {output_file}\")\n",
    "\n",
    "# Also save optimization results as JSON\n",
    "results_file = f\"results_worksheet_{WORKSHEET_INDEX}.json\"\n",
    "\n",
    "# Calculate scores from object attributes\n",
    "baseline_score = sum(baseline_result.scores) / len(baseline_result.scores) if baseline_result.scores else 0\n",
    "optimized_score = sum(optimized_result.scores) / len(optimized_result.scores) if optimized_result.scores else 0\n",
    "improvement = optimized_score - baseline_score\n",
    "improvement_pct = (improvement / baseline_score * 100) if baseline_score > 0 else 0\n",
    "\n",
    "results_data = {\n",
    "    \"worksheet_index\": WORKSHEET_INDEX,\n",
    "    \"baseline_score\": baseline_score,\n",
    "    \"optimized_score\": optimized_score,\n",
    "    \"improvement\": improvement,\n",
    "    \"improvement_percentage\": improvement_pct,\n",
    "    \"max_metric_calls\": 100,\n",
    "    \"task_lm\": \"gemini/gemini-2.0-flash-lite\",\n",
    "    \"reflection_lm\": \"openai/gpt-5\",\n",
    "    \"optimized_program\": optimization_result.best_candidate[\"program\"]\n",
    "}\n",
    "\n",
    "with open(results_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Optimization results saved to: {results_file}\")\n",
    "print(f\"\\n‚úÖ Worksheet {WORKSHEET_INDEX} optimization complete!\")\n",
    "print(f\"üìù To optimize another worksheet, change WORKSHEET_INDEX in cell 16 and run again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87285341",
   "metadata": {},
   "source": [
    "## Batch Optimization Helper (Optional)\n",
    "\n",
    "Use this cell to optimize multiple worksheet types in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9f6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_single_worksheet(worksheet_idx: int, max_iterations: int = 100, save_results: bool = True):\n",
    "    \"\"\"\n",
    "    Optimize a single worksheet type individually.\n",
    "    \n",
    "    Args:\n",
    "        worksheet_idx: Index of the worksheet to optimize (e.g., 1, 2, 3)\n",
    "        max_iterations: Number of GEPA optimization iterations\n",
    "        save_results: Whether to save optimized program and results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with optimization results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"OPTIMIZING WORKSHEET TYPE {worksheet_idx}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Load single worksheet\n",
    "    try:\n",
    "        item = load_dataset_item(worksheet_idx, \"Dataset\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading worksheet {worksheet_idx}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare dataset\n",
    "    dataset = prepare_dspy_dataset([item])\n",
    "    train_set = dataset\n",
    "    val_set = dataset\n",
    "    \n",
    "    print(f\"‚úì Loaded worksheet {worksheet_idx}\")\n",
    "    print(f\"  Images: {len(item['images'])}\")\n",
    "    print(f\"  Prompt: {item['prompt'][:100]}...\")\n",
    "    \n",
    "    # Baseline evaluation\n",
    "    print(f\"\\nüìä Evaluating baseline...\")\n",
    "    baseline = adapter.evaluate(val_set, {\"program\": seed_program})\n",
    "    baseline_score = sum(baseline.scores) / len(baseline.scores) if baseline.scores else 0\n",
    "    print(f\"  Baseline score: {baseline_score:.4f}\")\n",
    "    \n",
    "    # Run optimization\n",
    "    print(f\"\\nüöÄ Starting GEPA optimization ({max_iterations} iterations)...\")\n",
    "    result = optimize(\n",
    "        seed_candidate={\"program\": seed_program},\n",
    "        trainset=train_set,\n",
    "        valset=val_set,\n",
    "        adapter=adapter,\n",
    "        reflection_lm=lambda x: reflection_lm(x)[0],\n",
    "        max_metric_calls=max_iterations,\n",
    "        display_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Evaluate optimized\n",
    "    print(f\"\\nüìä Evaluating optimized program...\")\n",
    "    optimized = adapter.evaluate(val_set, result.best_candidate)\n",
    "    optimized_score = sum(optimized.scores) / len(optimized.scores) if optimized.scores else 0\n",
    "    \n",
    "    # Results\n",
    "    improvement = optimized_score - baseline_score\n",
    "    improvement_pct = (improvement / baseline_score * 100) if baseline_score > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RESULTS FOR WORKSHEET {worksheet_idx}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Baseline:   {baseline_score:.4f}\")\n",
    "    print(f\"Optimized:  {optimized_score:.4f}\")\n",
    "    print(f\"Improvement: {improvement:+.4f} ({improvement_pct:+.2f}%)\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Save if requested\n",
    "    if save_results:\n",
    "        # Save optimized program\n",
    "        program_file = f\"optimized_worksheet_{worksheet_idx}.py\"\n",
    "        with open(program_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(result.best_candidate[\"program\"])\n",
    "        print(f\"üíæ Saved program: {program_file}\")\n",
    "        \n",
    "        # Save results\n",
    "        results_file = f\"results_worksheet_{worksheet_idx}.json\"\n",
    "        results_data = {\n",
    "            \"worksheet_index\": worksheet_idx,\n",
    "            \"baseline_score\": baseline_score,\n",
    "            \"optimized_score\": optimized_score,\n",
    "            \"improvement\": improvement,\n",
    "            \"improvement_percentage\": improvement_pct,\n",
    "            \"max_iterations\": max_iterations,\n",
    "            \"task_lm\": \"gemini/gemini-2.0-flash-lite\",\n",
    "            \"reflection_lm\": \"openai/gpt-5\",\n",
    "            \"optimized_program\": result.best_candidate[\"program\"]\n",
    "        }\n",
    "        with open(results_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results_data, f, indent=2)\n",
    "        print(f\"üíæ Saved results: {results_file}\")\n",
    "    \n",
    "    return {\n",
    "        \"worksheet_idx\": worksheet_idx,\n",
    "        \"baseline\": baseline_score,\n",
    "        \"optimized\": optimized_score,\n",
    "        \"improvement\": improvement,\n",
    "        \"program\": result.best_candidate[\"program\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# BATCH OPTIMIZATION: Optimize multiple worksheets\n",
    "# ============================================================\n",
    "# Uncomment and run this to optimize multiple worksheets sequentially\n",
    "\n",
    "# worksheet_indices = [1, 2, 3]  # Add all worksheet indices you want to optimize\n",
    "# results_summary = []\n",
    "# \n",
    "# for idx in worksheet_indices:\n",
    "#     result = optimize_single_worksheet(\n",
    "#         worksheet_idx=idx,\n",
    "#         max_iterations=100,  # Adjust as needed\n",
    "#         save_results=True\n",
    "#     )\n",
    "#     if result:\n",
    "#         results_summary.append(result)\n",
    "#     print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "# \n",
    "# # Print summary\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"OPTIMIZATION SUMMARY FOR ALL WORKSHEETS\")\n",
    "# print(\"=\"*80)\n",
    "# for r in results_summary:\n",
    "#     print(f\"Worksheet {r['worksheet_idx']}: {r['baseline']:.4f} ‚Üí {r['optimized']:.4f} ({r['improvement']:+.4f})\")\n",
    "# print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdb1918",
   "metadata": {},
   "source": [
    "## Compare Results Across Worksheets\n",
    "\n",
    "View optimization results for all worksheets you've processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9b0ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_worksheets():\n",
    "    \"\"\"\n",
    "    Load and compare results from all optimized worksheets.\n",
    "    Displays a summary table of improvements.\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\" \"*35 + \"WORKSHEET OPTIMIZATION SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'Worksheet':<12} {'Baseline':<12} {'Optimized':<12} {'Improvement':<15} {'% Change':<12} {'Status':<10}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    # Find all result files\n",
    "    result_files = sorted(glob.glob(\"results_worksheet_*.json\"))\n",
    "    \n",
    "    if not result_files:\n",
    "        print(\"No optimization results found. Run optimizations first.\")\n",
    "        return\n",
    "    \n",
    "    total_improvements = []\n",
    "    \n",
    "    for result_file in result_files:\n",
    "        try:\n",
    "            with open(result_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            ws_idx = data.get('worksheet_index', 'N/A')\n",
    "            baseline = data.get('baseline_score', 0)\n",
    "            optimized = data.get('optimized_score', 0)\n",
    "            improvement = data.get('improvement', 0)\n",
    "            pct_change = data.get('improvement_percentage', 0)\n",
    "            \n",
    "            # Status indicator\n",
    "            if improvement > 0.1:\n",
    "                status = \"üü¢ Great\"\n",
    "            elif improvement > 0:\n",
    "                status = \"üü° Good\"\n",
    "            else:\n",
    "                status = \"üî¥ Review\"\n",
    "            \n",
    "            total_improvements.append(improvement)\n",
    "            \n",
    "            print(f\"{'WS-' + str(ws_idx):<12} {baseline:<12.4f} {optimized:<12.4f} {improvement:+<15.4f} {pct_change:+<12.2f}% {status:<10}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {result_file}: {e}\")\n",
    "    \n",
    "    print(\"-\"*100)\n",
    "    if total_improvements:\n",
    "        avg_improvement = sum(total_improvements) / len(total_improvements)\n",
    "        print(f\"Average Improvement: {avg_improvement:+.4f} across {len(total_improvements)} worksheet(s)\")\n",
    "    print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "\n",
    "# Run this to see comparison of all optimized worksheets\n",
    "# compare_all_worksheets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9478b5",
   "metadata": {},
   "source": [
    "## Summary and Usage Guide\n",
    "\n",
    "### üéØ Single-Worksheet Optimization Mode\n",
    "\n",
    "This notebook is configured for **individual worksheet optimization** - optimizing prompts for one specific worksheet type at a time, without generalization.\n",
    "\n",
    "### What This Does:\n",
    "\n",
    "1. **Loads Single Worksheet**: Reads one specific worksheet (e.g., worksheet 1, 2, or 3)\n",
    "2. **Focused Optimization**: GEPA optimizes specifically for that worksheet type\n",
    "3. **Multiple Iterations**: Uses reasoning model (GPT-5) to iteratively improve the prompt\n",
    "4. **Saves Results**: Each worksheet gets its own optimized program file\n",
    "5. **No Generalization**: Each worksheet type is treated independently\n",
    "\n",
    "### üìã Workflow:\n",
    "\n",
    "#### **Option 1: Optimize One Worksheet at a Time (Manual)**\n",
    "\n",
    "1. **Set the worksheet index** in cell 16:\n",
    "   ```python\n",
    "   WORKSHEET_INDEX = 1  # Change to 2, 3, etc. for other worksheets\n",
    "   ```\n",
    "\n",
    "2. **Run all cells** to optimize that specific worksheet\n",
    "\n",
    "3. **Results saved as**:\n",
    "   - `optimized_program.py` - The optimized DSPy program\n",
    "   - `optimization_results.json` - Performance metrics\n",
    "\n",
    "4. **Repeat for other worksheets**:\n",
    "   - Change `WORKSHEET_INDEX = 2`\n",
    "   - Run all cells again\n",
    "   - Continue for worksheet 3, 4, etc.\n",
    "\n",
    "#### **Option 2: Batch Optimize Multiple Worksheets (Automated)**\n",
    "\n",
    "Use the batch helper function in the last cell:\n",
    "\n",
    "```python\n",
    "# Optimize worksheets 1, 2, and 3 sequentially\n",
    "worksheet_indices = [1, 2, 3]\n",
    "results_summary = []\n",
    "\n",
    "for idx in worksheet_indices:\n",
    "    result = optimize_single_worksheet(\n",
    "        worksheet_idx=idx,\n",
    "        max_iterations=100,  # More iterations = better optimization\n",
    "        save_results=True\n",
    "    )\n",
    "    if result:\n",
    "        results_summary.append(result)\n",
    "\n",
    "# View summary of all optimizations\n",
    "for r in results_summary:\n",
    "    print(f\"Worksheet {r['worksheet_idx']}: {r['baseline']:.4f} ‚Üí {r['optimized']:.4f}\")\n",
    "```\n",
    "\n",
    "**Saves for each worksheet**:\n",
    "- `optimized_worksheet_1.py`\n",
    "- `results_worksheet_1.json`\n",
    "- `optimized_worksheet_2.py`\n",
    "- `results_worksheet_2.json`\n",
    "- etc.\n",
    "\n",
    "### üîß Key Configuration:\n",
    "\n",
    "**Language Models** (cell 18):\n",
    "- Task LLM: `Gemini 2.0 Flash Lite` (processes images and executes tasks)\n",
    "- Reflection LLM: `GPT-5` (reasons about improvements)\n",
    "\n",
    "**Optimization Iterations** (cell 24 or in batch function):\n",
    "- `max_metric_calls=100` - More iterations = better optimization\n",
    "- Adjust based on time/budget\n",
    "\n",
    "### üìÅ Dataset Structure:\n",
    "\n",
    "```\n",
    "Dataset/\n",
    "‚îú‚îÄ‚îÄ prompts/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 1.txt  (worksheet type 1)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 2.txt  (worksheet type 2)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 3.txt  (worksheet type 3)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ outputs/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 1.json (expected output for worksheet 1)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 2.json\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ images/\n",
    "    ‚îú‚îÄ‚îÄ 1-a.png, 1-b.jpg (images for worksheet 1 - PNG or JPG)\n",
    "    ‚îú‚îÄ‚îÄ 2-a.jpg, 2-b.jpeg (images for worksheet 2)\n",
    "    ‚îî‚îÄ‚îÄ ...\n",
    "```\n",
    "\n",
    "**Image formats supported**: PNG, JPG, JPEG (case-insensitive)\n",
    "\n",
    "### üí° Why This Approach?\n",
    "\n",
    "- ‚úÖ **Specialized optimization** for each worksheet type\n",
    "- ‚úÖ **No generalization needed** - each prompt is unique\n",
    "- ‚úÖ **Deep optimization** - multiple reasoning iterations on single type\n",
    "- ‚úÖ **Easy iteration** - optimize worksheet 1, then 2, then 3, etc.\n",
    "- ‚úÖ **Individual tracking** - separate results for each worksheet\n",
    "\n",
    "### üöÄ Quick Start:\n",
    "\n",
    "1. Set `WORKSHEET_INDEX = 1` in cell 16\n",
    "2. Click \"Run All\"\n",
    "3. Wait for optimization to complete\n",
    "4. Check `optimized_worksheet_1.py` for results\n",
    "5. Repeat for other worksheets!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
